
==== Front
Entropy (Basel)
Entropy (Basel)
entropy
Entropy
1099-4300
MDPI

10.3390/e23111427
entropy-23-01427
Article
On Conditional Tsallis Entropy
https://orcid.org/0000-0003-1199-2220
Teixeira Andreia 123
https://orcid.org/0000-0001-8792-959X
Souto André 456*
https://orcid.org/0000-0002-9988-594X
Antunes Luís 7
Broadbridge Philip Academic Editor
1 CINTESIS—Centre for Health Technology and Services Research, Faculty of Medicine, University of Porto, 4200-450 Porto, Portugal; andreiasofiat@med.up.pt
2 MEDCIDS—Department of Community Medicine, Information and Decision in Health, Faculty of Medicine, University of Porto, 4200-450 Porto, Portugal
3 ADiT-LAB, Instituto Politécnico de Viana do Castelo, Rua Escola Industrial e Comercial Nun’Álvares, 4900-347 Viana do Castelo, Portugal
4 LASIGE, Faculdade de Ciências da Universidade de Lisboa, Campo Grande, 1749-016 Lisboa, Portugal
5 Departamento de Informática, Faculdade de Ciências da Universidade de Lisboa, Campo Grande, 1749-016 Lisboa, Portugal
6 Instituto de Telecomunicações, Av. Rovisco Pais, n 1, 1049-001 Lisboa, Portugal
7 Computer Science Department, Faculty of Sciences, University of Porto, Rua do Campo Alegre, 4169-007 Porto, Portugal; lfa@dcc.fc.up.pt
* Correspondence: ansouto@fc.ul.pt
29 10 2021
11 2021
23 11 142714 9 2021
27 10 2021
© 2021 by the authors.
2021
https://creativecommons.org/licenses/by/4.0/ Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
There is no generally accepted definition for conditional Tsallis entropy. The standard definition of (unconditional) Tsallis entropy depends on a parameter α that converges to the Shannon entropy as α approaches 1. In this paper, we describe three proposed definitions of conditional Tsallis entropy suggested in the literature—their properties are studied and their values, as a function of α, are compared. We also consider another natural proposal for conditional Tsallis entropy and compare it with the existing ones. Lastly, we present an online tool to compute the four conditional Tsallis entropies, given the probability distributions and the value of the parameter α.

Tsallis entropy
conditional Tsallis entropies
Generalizations of Shannon entropy
==== Body
pmc1. Introduction

Tsallis entropy [1], (The name Tsallis entropy used in this paper, to identify the quantity presented in Equation (3), is not consensual in the community, given that before Tsallis presented it in 1988, and as he himself acknowledges, other authors had already introduced it [2,3,4].) a generalization of Shannon entropy [5,6], was extensively studied by Constantino Tsallis in 1988, and provides an alternative way of dealing with several characteristics of nonextensive physical systems, given that the information about the intrinsic fluctuations in the physical system can be characterized by the nonextensivity parameter α. It can be applied to many scientific fields, such as physics [7], economics [8], computer science [9,10], and biology [11]. We refer the reader to Reference [12] for a more extensive bibliography on applications of Tsallis entropy. Furthermore, we refer the reader to Reference [13] for a survey on the most significant areas of application of the most usual entropy measures, including Shannon [6], Rényi [14], and Tsallis entropies [1,2,3,4].

It is known that, as the parameter α approaches 1, the Tsallis entropy corresponds to the Shannon entropy. Unlike for Shannon entropy, but similar to Rényi entropy (yet another generalization of Shannon entropy developed by Alfréd Rényi in 1961 [14], which also depends on a parameter α and converges to Shannon entropy when α approaches 1), there is no commonly accepted definition for the conditional Tsallis entropy: several versions have been proposed and used in the literature [15,16]. In this work, we revisit the notion of conditional Tsallis entropy by studying some natural and desirable properties in the existing proposals (see for instance References [15,16]): when α→1, the usual conditional Shannon entropy should be recovered, the conditional Tsallis entropy should not exceed the unconditional Tsallis entropy, and the conditional Tsallis entropy should have values between 0 and the maximum value of the unconditional version.

The use of entropies in different fields, especially in the field of information theory and its connection to communication, allowed the development of several useful information measures, such as mutual information, symmetry of information, and information distances. See, for example, References [17,18,19] for some recent work related to the aforementioned information measures.

Depending on the entropy measure used, all of these have been applied in many different areas of knowledge, such as physics [20], information theory [21,22], complexity theory [23,24,25], security [26,27,28], biology [29,30,31,32], finances [33], and medicine [34,35,36], among others. The conditional Tsallis entropy, as suggested in Reference [37], can be directly applied to information theory, especially coding theory. Furthermore, since Tsallis entropy can be applied in many areas (see, for example, Reference [12]), the study of conditional Tsallis entropies is quite promising. This paper analyzes several definitions of conditional Tsallis entropy, with the intent of providing the reader with a description of the properties that each approach satisfies.

Continuing from previous works [37,38], we introduce a new natural definition for conditional Tsallis entropy as a possible alternative to the existing ones. Our new proposal does not intend to be the ultimate version of conditional Tsallis entropy, but an alternative to the existing ones, with its own properties that, in settings, such as biomedical applications, might be useful for defining information distances or other significant measurements. None of the known definitions contain all of the desired properties for a conditional version. In particular, the one presented here (as it takes the maximum over the marginal distributions) does not converge to the Shannon entropy when α→1—it behaves similar to a parameterized entropy, and is akin to the one proposed in Reference [38] as an alternative to Rényi’s conditional entropy, another generalization of Shannon entropy.

The paper is organized as follows. In the next section, we present the definitions necessary for the rest of the paper, namely Shannon entropy and Tsallis entropy. In Section 3, we provide several definitions for the conditional Tsallis entropy in both existing literature and our proposal. In Section 4, we establish several results, comparing the definitions presented previously. In Section 5, we explore some features of each variant for the conditional Tsallis entropy. Finally, in Section 6, we present the conclusions and future work.

2. Preliminaries

In the remainder of the paper, we use the standard notation for entropies and for probability distributions according to Reference [5]. For the sake of simplicity of notation, we use the notation log for the logarithm in base 2. We call the reader’s attention to the fact that, whenever we say that one entropy converges to another, it is always up to logarithmic factor that depends only on the choice of cardinality of the alphabet.

The Shannon entropy of X is the expectation of the surprise of an occurrence, (1) H(X)=−∑xP(X=x)logP(X=x).

The conditional Shannon entropy, H(Y|X), is the expectation over x of the entropy of the distribution P(Y|X=x), (2) H(Y|X)=Ex,ylog1P(Y=y|X=x).

It is easy to derive the chain rule H(X,Y)=H(X)+H(Y|X): to get the average information contained in (X,Y), we may first get the average information contained in X, and add to it the average information of Y, given X.

The Tsallis entropy [1] was firstly introduced in [2,3] and is defined for a random variable X by:(3) Tα(X)=1α−11−∑xP(X=x)α,(forα>0,α≠1).

It is straightforward to show that, when the parameter α converges to 1, the value of the entropy converges to the Shannon entropy.

3. Conditional Tsallis Entropy: Four Definitions

We consider three definitions for conditional Tsallis entropy that already exist in the literature and introduce a new proposal. All definitions consider a positive parameter α.

Definition 1. Let Z=(X,Y) be a random vector. One can define the following variants of conditional Tsallis entropy: 1.  Definition of Tα(Y|X) from Reference [15] (4) Tα(Y|X)=∑xP(X=x)αTα(Y|x)

(5) =1α−1∑xP(X=x)α1−∑yP(Y=y|X=x)α.

One can easily verify that Tα(X,Y)=Tα(Y|X)+Tα(X) and, therefore, it satisfies the chain rule.

2.  Definition of Sα(Y|X) from [16] (Definition 2.8) (6) Sα(Y|X)=∑xP(X=x)Tα(Y|X=x)

(7) =∑xP(X=x)1α−11−∑yP(Y=y|X=x)α

(8) =1α−1∑xP(X=x)1−∑yP(Y=y|X=x)α.

3.  Definition of Sα′(Y|X) from [16] (Definition 2.10) (9) Sα′(Y|X)=1α−11−∑x,yP(X=x,Y=y)α∑xP(X=x)α.

The first definition presented proposes that the conditional Tsallis entropy should be weighed by the probability of sampling X=x with parameter α, while the second one proposes that one uniformly weighs only the probability of sampling X=x. Therefore, notice that for the first definition presented, the value of α largely affects the value of the conditional Tsallis entropy. The idea for the third proposal is to distribute evenly the influence of the parameter α by the entire joint distribution.

Next, we present another possible definition of the conditional Tsallis entropy. This definition is based on Definition III.6 of [38] and captures the intuitive notion of defining the conditional entropy, by taking the maximum over all possible marginal distributions. Note that this definition is analogous to an existing one for the Rényi entropy; however, as we will show later, this proposal does not satisfy some of the expected basic properties.

Definition 2 (Definition of Tα′(Y|X)). (10) Tα′(Y|X)=1α−1maxx1−∑yP(Y=y|X=x)α.

We opted to use different notations for the variants of the conditional Tsallis entropy in the last definition, to better distinguish them in the rest of the paper. In particular, we follow the same approach as in Reference [38].

The following expressions will be useful later.

Theorem 1. Let Z=(X,Y) be a random vector. The following identities are true: (11) Tα′(Y|X)=maxxTα(Y|X=x)(forα>1)

(12) Tα′(Y|X)=minxTα(Y|X=x)(forα<1).

4. Comparison of the Definitions

We now compare the above four definitions of the conditional Tsallis entropy by comparing whether or not the definition satisfies some common properties of an entropy measure. In the next theorem, we report two simple facts with straightforward proofs. We leave the details for the interested reader to check.

Theorem 2. For any fixed joint probability distribution P(X,Y), (i)  Tα(Y|X), Sα(Y|X) and Sα′(Y|X), as functions of α, are continuous and differentiable;

(ii)  Tα′(Y|X), as a function of α, is continuous for all α≠1.

The following results provide the possible comparisons (in terms of values) between the proposed definitions. For the sake of organization, we split the comparison by types of entropy.

First we compare Tα(Y|X) with Sα(Y|X).

Theorem 3. For all joint probability distributions P(X,Y) and for every α>0, (13) ifα<1:Sα(Y|X)≤Tα(Y|X)

(14) ifα=1:Sα(Y|X)=Tα(Y|X)=H(Y|X)

(15) ifα>1:Sα(Y|X)≥Tα(Y|X).

Proof.  Consider first the case α<1. In this case, we have that P(X=x)α≥P(X=x). Thus, P(X=x)α×Tα(Y|X=x)≥P(X=x)×Tα(Y|X=x)⇔∑xP(X=x)α×Tα(Y|X=x)≥∑xP(X=x)×Tα(Y|X=x)⇔Tα(Y|X)≥Sα(Y|X).

For the case α=1, see the proof of Theorem 8.

The case α>1 is similar to the previous one, but this time, the conclusion follows, since for α>1, P(X=x)α≤P(X=x). □

In the next theorem we provide the comparison between Tα′(Y|X) and Sα(Y|X).

Theorem 4. For all joint probability distributions P(X,Y) and for every α>0, (16) ifα≤1:Tα′(Y|X)≤Sα(Y|X)

(17) ifα>1:Tα′(Y|X)≥Sα(Y|X).

Proof.  Consider first the case α<1. In this case, we have that Tα′(Y|X)=minxTα(Y|X=x). So, (18) Sα(Y|X)=∑xP(X=x)·Tα(Y|X=x)

(19) ≥∑x(P(X=x)·minxTα(Y|X=x))

(20) =minxTα(Y|X=x)·∑xP(X=x)

(21) =minxTα(Y|X=x)

(22) =Tα′(Y|X).

The proof of the case α>1 is similar to the previous one but this time, the conclusion follows from the fact that, for α>1, Tα′(Y|X)=maxxTα(Y|X=x). □

As a consequence of the two previous results and the definitions, we can derive the relation between Tα′ and Tα.

Corollary 1. For all joint probability distributions P(X,Y) and for every α>0, (23) ifα≤1:Tα′(Y|X)≤Tα(Y|X)

(24) ifα>1:Tα′(Y|X)≥Tα(Y|X).

The proof follows directly from Theorems 3 and 4. Now, we derive the relation between Sα′(Y|X) and Tα′(Y|X).

Theorem 5. For all joint probability distributions P(X,Y) and for every α>0, (25) ifα≤1:Sα′(Y|X)≥Tα′(Y|X)

(26) ifα>1:Sα′(Y|X)≤Tα′(Y|X).

Proof.  Consider first the case α<1. Proving that Sα′(Y|X)≥Tα′(Y|X), by definition, is the same to prove: (27) 1−∑x,yP(X=x,Y=y)α∑xP(x)αα−1≥maxx1−∑yP(y|x)αα−1.

As α<1, we have that 1α−1<0. Thus, proving Equation (27) is the same, proves that: 1−∑x,yP(X=x,Y=y)α∑xP(X=x)α≤maxx1−∑yP(Y=y|X=x)α⇔∑x,yP(X=x,Y=y)α∑xP(X=x)α≥minx∑yP(Y=y|X=x)α⇔∑x,yP(X=x,Y=y)α≥∑xP(X=x)α×minx∑yP(Y=y|X=x)α.

Now, the result follows by observing that the last inequality is true, since, for α<1 and for every x, we have that minx∑yP(Y=y|X=x)α≤∑yP(Y=y,X=x)α.

The case α>1 is proved in a similar manner. □

Now, we derive the relation between Tα(Y|X) and Sα′(Y|X).

Theorem 6. For all joint probability distributions P(X,Y) and for every α>0, (28) ifα≤1:Tα(Y|X)≥Sα′(Y|X)

(29) ifα>1:Tα(Y|X)≤Sα′(Y|X).

Proof.  Consider first the case α<1. Thus, Tα(Y|X)≥Sα′(Y|X)⇔1α−1∑xP(X=x)α1−∑yP(Y=y|X=x)α≥1α−11−∑x,yP(X=x,Y=y)α∑xP(X=x)α

⇔∑xP(X=x)α1−∑yP(Y=y|X=x)α≤1−∑x,yP(X=x,Y=y)α∑xP(X=x)α⇔∑xP(X=x)α−∑xP(X=x)α∑yP(X=x,Y=y)αP(X=x)α≤1−∑x,yP(X=x,Y=y)α∑xP(X=x)α⇔∑xP(X=x)α−∑x,yP(X=x,Y=y)α≤∑xP(X=x)α−∑x,yP(X=x,Y=y)α∑xP(X=x)α.

The result follows by observing that the last inequality is true, since for α<1, we have that: (30) P(X=x)α>P(X=x)

and consequently, (31) ∑xP(X=x)α>1.

The proof of the case α>1 is similar to the previous one. □

Finally, we show that the values of Sα and Sα′ are incomparable in the sense that there are probability distributions for which Sα is greater than Sα′ and there are probability distributions for which Sα′ is greater than Sα.

Theorem 7. The values of Sα(Y|X) and of Sα′(Y|X) are incomparable, i.e., for each n≥2 and α≠1 (32) ∃P(X,Y):Sα(Y|X)<Sα′(Y|X)

(33) ∃P(X,Y):Sα(Y|X)>Sα′(Y|X).

Proof.  For Statement (32) and α<1, consider the following joint probability distribution: X\Y1210.06250.062520.01250.8625

(34) S0.25(Y|X)≈0.513

(35) S0.25′(Y|X)≈0.629

For Statement (32) and α>1, consider the following joint probability distribution: X\Y1210.11250.012520.43750.4375

(36) S2.5(Y|X)≈0.396

(37) S2.5′(Y|X)≈0.429

For Statement (33) and α<1, consider the following joint probability distribution: X\Y1210.125020.50.375

(38) S0.25(Y|X)≈0.792

(39) S0.25′(Y|X)≈0.560

Finally, for Statement (33) and α>1, consider the following joint probability distribution: X\Y1210.06250.062520.01250.8625

(40) S1.25(Y|X)≈0.125

(41) S1.25′(Y|X)≈0.099

□

5. Properties of the Conditional Tsallis Entropies

In this section, we investigate some properties of the proposals considered. In particular, we show that there are probability distributions and α≠1 for which the conditional Tsallis entropies are bigger than the unconditional Tsallis entropy.

Theorem 8. For any fixed joint probability distribution P(X,Y), (42) limα→1Tα(Y|X)=H(Y|X)

(43) limα→1Sα(Y|X)=H(Y|X)

(44) limα→1Sα′(Y|X)=H(Y|X)

where H(Y|X) is the conditional Shannon entropy. In general, it is not true that limα→1Tα′(Y|X)=H(Y|X).

Proof.  The second equation is easy to derive directly from the definition of conditional probability and from Equation (2). Furthermore, using Equation (6) we can also easily obtain (using the previous derivation) that Equation (42) is also true.

The third equation was proven in Reference [16].

Now, it is only left to prove the last statement of the theorem, i.e., in general (45) limα→1Tα′(Y|X)≠H(Y|X).

From Equations (6) and (11) it is easy to check that Tα(Y|X) is the expectation over x of Tα(Y|x), while Tα′(Y|X) is the maximum over x of Tα(Y|x).

The function Tα(Y|x) depends on the conditional probabilities P(Y=y|X=x). Therefore, there are joint probability distributions P(X=x,Y=y), such that: (46) limα→1Tα′(Y|X)≠limα→1Tα(Y|X)=H(Y|X).

□

Contrary to the Shannon entropy, the value of any conditional Tsallis entropy may exceed the corresponding unconditional Tsallis entropy for all proposals.

Theorem 9. There are probability distributions P(X,Y) and values of α, such that: (47) Tα(Y|X)>Tα(Y)

(48) Sα(Y|X)>Tα(Y)

(49) Sα′(Y|X)>Tα(Y)

(50) Tα′(Y|X)>Tα(Y).

Proof.  Consider the following joint probability distribution: X=x\Y=y1210.450.4520.10.0

For this distribution we have: (51) T0.5(Y)≈0.824

(52) T0.5(Y|X)≈1.047

(53) S0.5(Y|X)≈0.828

(54) T3(Y)≈0.371

(55) S3′(Y|X)≈0.374

(56) T3′(Y|X)≈0.375

□

Bounds on Conditional Tsallis Entropy

As mentioned in the Introduction, one of the properties of the (conditional) Shannon entropy for discrete variables is to be bounded by the number of elements of the support of the distribution. Furthermore, it is well known that the unconditional Tsallis entropy is always between 0 and m1−α1−α, where m is the number of elements in the support of the distribution. In this subsection, we derive bounds for the conditional Tsallis entropies based on the number of elements in the support of each distribution.

Theorem 10. Let Z=(X,Y) be any joint random vector defined over sets of size m each. Then, (57) 0≤Sα(Y|X)≤m1−α1−α

(58) 0≤Tα′(Y|X)≤m1−α1−α.

Moreover all of these lower and upper bounds may be reached by suitable probability distributions P(X,Y).

Proof.  The Inequalities (57) follow from the fact that Sα(Y|X) is the expectation of the unconditional Tsallis entropy.

For Inequalities (58), recall that Equation (10) can be written, for α<1, as Equation (12). Note that, for all x, the values Tα(Y|X=x) are the (unconditional) Tsallis entropies of the marginal distribution, and are all defined in a set of cardinality m.

So, by definition of Tα′, for some particular x, we have Tα′(Y|X)=Tα(Y|X=x). The case α>1 is similar. So, independently of α, for every probability distributions P(X) and P(Y) defined over set with m elements, we have 0≤Tα′(Y|X)≤m1−α1−α, since the same bound applies for the unconditional version or any its marginal distributions. □

Theorem 11. Let Z=(X,Y) be any joint random vector defined over sets of size m each. Then, (59) ifα>1:0≤Tα(Y|X)≤m1−α1−α.

For α<1, in general, the inequality does not hold.

Proof.  Consider first the case α>1. The result follows directly from Inequalities (15) and (57).

In order to prove that the inequality does not hold for all α<1, consider α=0.1 and the following joint probability distribution: X=x\Y=y12311/91/9021/91/91/9301/91/3

Notice that m1−α1−α≈2.987 and T0.1(Y|X)≈3.371. For any other α<1, one can construct similarly a joint probability distribution for which the inequality is also violated. □

Theorem 12. Let Z=(X,Y) be any joint random vector defined over sets of size m each. Then, (60) ifα>1:0≤Sα′(Y|X)≤m1−α1−α.

Proof.  The result follows directly from the Inequalities (26) and (58). □

We conjecture that the above theorem also holds for α<1. For example, the inequality is true for all uniform probability distribution over n variables.

We now show that, for any fixed joint probability distribution P(X,Y), three of the forms of conditional Tsallis entropy studied in this paper are non-increasing functions of α. First, we state a simple theorem.

Lemma 1. If f1(x),…, fm(x) are non-increasing real functions, then the function maxi(fi(x)) is also a non-increasing function.

Theorem 13. For every probability distribution P(X,Y), 1.  Tα(Y|X) is a non-increasing function of α.

2.  Sα(Y|X) is a non-increasing function of α.

3.  Tα′(Y|X) is a non-increasing function of α.

Proof.  1. First consider the case α>1, and consider the function dTα(Y|X), the derivative of the function Tα(Y|X) in order to α: dTα(Y|X)dα=−1+∑xP(X=x)α(α−1)2−∑xαP(Y=y|X=x)α−1logαα−1.

It is easy to see that, since α>1, dTα(X)dα<0. Therefore, the function Tα(Y|X) is a non-increasing function of α.

Consider now the case α<1 and assume that α,α′ are such that α<α′<1. In order to prove that Tα(Y|X) is non-increasing we have to show that Tα(X)≥Tα′(X), i.e.,: 1−1−∑xP(X=x)αα−1≥1−1−∑xP(X=x)α′α′−1⇔−1+∑xP(X=x)α1−α≥−1+∑xP(X=x)α′1−α′⇔1−∑xP(X=x)α1−α≤1−∑xP(X=x)α′1−α′

Notice that, since α<α′<1, Then 11−α<11−α′ and, therefore, 1−∑xP(X=x)α≤1−∑xP(X=x)α′. So, the last inequality is true. 2. This part of the result follows from the fact that Sα(Y|X) is the expectation of unconditional Tsallis entropies; see Equation (6).

3. Suppose that α>1. The proof is a direct consequence of Equation (11) and Lemma 1. The case α<1 can be proven in a similar way.

□

It is easy to show that S′ does not fulfill the property of the last theorem.

Theorem 14. There exists probability distributions (X,Y) and α<α′ for which Sα′(Y|X)≤Sα′′(Y|X).

Proof.  Consider the following joint probability distribution: X=x\Y=y1210.450.4520.10.0

We have: S0.2′(Y|X)≈0.563

S0.5′(Y|X)≈0.621.

□

We developed a small application that, given two probability distributions, computes the values of all conditional Tsallis entropies considered in the paper. The application is self-contained and its use is extremely simple. There are two use case examples that the reader can use in order to try the calculator. The interested reader can find it in the following link: http://gloss.di.fc.ul.pt/tryit/Tsallis (accessed on 28 October 2021).

6. Conclusions

In this paper, we studied the definitions for the conditional Tsallis entropy existing in the literature. We also considered a possible alternative definition for it. This new proposal is a natural approach to consider as a possible definition. It defines the conditional value as the maximum value of all marginal distributions. Due to this fact, and similar to what happens with the Rényi entropy, this definition was also analyzed, although it was never considered in the literature before. The relationships between the four definitions, described in this work, are summarized in Figure 1.

As we understand, it would be expectable that a proposal for conditional Tsallis entropy would satisfy the following properties:Chain Rule;

Convergence to Shannon entropy as the parameter α tended to 1;

Its value would be between 0 and the upper bound of the unconditional version.

In Table 1, we summarize the properties that the four proposals have (we also added the property of being a non-increasing function with α). To conclude, we can say that none of the proposals fulfill all of the properties. The definition Tα(Y|X) is the candidate that fulfills more properties.

For future work, since all definitions focus on possible different aspects of the entropy, it would be important to consider a deeper study in this area and its possible applications, aiming to develop a theory that would emphasize the best proposal for each area, or eventually present an ultimate version for the conditional Tsallis entropy that would satisfy all of the desirable properties.

Author Contributions

Conceptualization, A.T., A.S. and L.A.; methodology, A.T., A.S. and L.A.; validation, A.T., A.S. and L.A.; formal analysis, A.T. and A.S.; investigation, A.T., A.S. and L.A.; writing—original draft preparation, A.T. and A.S.; writing—review and editing A.T., A.S. and L.A. All authors have read and agreed to the published version of the manuscript.

Funding

This research was supported by FCT—Fundação para a Ciência e a Tecnologia, within CINTESIS, R&D Unit (reference UIDB/4255/2020), within Instituto de Telecomunicações (IT) Research Unit ref. UIDB/EEA/50008/2020 and within LASIGE Research Unit, ref. UIDB/00408/2020 and ref. UIDP/00408/2020. It was also supported by the projects Predict PTDC/CCI-CIF/29877/2017, QuantumMining POCI-01-0145-FEDER-031826 funded by FCT through national funds, by the European Regional Development Fund (FEDER), through the Competitiveness and Internationalization Operational Programme (COMPETE 2020), from EU H2020-SU-ICT-03-2018 project no. 830929 CyberSec4Europe (cybersec4europe.eu), and also the project “Safe Cities”, reference POCI-01-0247-FEDER-041435, financed by Fundo Europeu de Desenvolvimento Regional (FEDER),through COMPETE 2020 and Portugal 2020.

Institutional Review Board Statement

Not applicable.

Informed Consent Statement

Not applicable.

Conflicts of Interest

The authors declare no conflict of interest.

Figure 1 Summary of the relations between the several proposals for the definition of conditional Tsallis entropy.

entropy-23-01427-t001_Table 1 Table 1 Summary of the proved properties of all proposed conditional entropies. The question mark indicates that the property is not known to be fulfilled.

f(Y|X)	Tα(Y|X)	Sα(Y|X)	Sα′(Y|X)	Tα′(Y|X)	
Chain Rule	yes	no	no	no	
limα→1f(Y|X)=H(Y|X)	yes	yes	yes	no	
0≤f(Y|X)≤|Y|1−α1−α and α>1	yes	yes	yes	yes	
0≤f(Y|X)≤|Y|1−α1−α and α<1	no	yes	?	yes	
f is non-increasing with α	yes	yes	no	yes	

Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.
==== Refs
References

1. Tsallis C. Possible generalization of Boltzmann-Gibbs statistics J. Stat. Phys. 1988 52 479 487 10.1007/BF01016429
2. Daróczy Z. Generalized information functions Inf. Control 1970 16 36 51 10.1016/S0019-9958(70)80040-7
3. Havrda J. Charvat F. Quantification method of classification processes. concept of structural α-entropy IEEE Trans. Inf. Theory 1967 3 30 35
4. Wehrl A. General properties of entropy Rev. Mod. Phys. 1978 50 221 260 10.1103/RevModPhys.50.221
5. Cover T. Thomas J.A. Elements of Information Theory 2nd ed. Wiley Hoboken, NJ, USA 2006
6. Shannon C.E. A Mathematical Theory of Communication Bell Syst. Tech. J. 1948 27 379 423 379–423, 623–656 10.1002/j.1538-7305.1948.tb01338.x
7. Tsallis C. The Nonadditive Entropy Sq and Its Applications in Physics and Elsewhere: Some Remarks Entropy 2011 13 1765 1804 10 10.3390/e13101765
8. Borland R.O.L. Tsallis C. Distributions of high-frequency stock-market observables Nonextensive Entropy—Interdisciplinary Applications Gell-Mann M. Tsallis C. Oxford University Press New York, NY, USA 2004
9. Ibrahim R.W. Darus M. Analytic Study of Complex Fractional Tsallis’ Entropy with Applications in CNNs Entropy 2018 20 722 10.3390/e20100722
10. Mohanalin B. Kalra P.K. Kumar N. A novel automatic microcalcification detection technique using Tsallis entropy and a type II fuzzy index Comput. Math. Appl. 2010 60 2426 2432
11. Tamarit F.A. Cannas S.A. Tsallis C. Sensitivity to initial conditions in the Bak-Sneppen model of biological evolution Eur. Phys. J. B 1998 1 545 548 10.1007/s100510050217
12. Group of Statistical Physics Available online: http://tsallis.cat.cbpf.br/biblio.htm (accessed on 8 November 2018)
13. Ribeiro M. Henriques T. Castro L. Souto A. Antunes L. Costa-Santos C. Teixeira A. The Entropy Universe Entropy 2021 23 222 10.3390/e23020222 33670121
14. Rényi A. On measures of information and entropy Berkeley Symp. Math. Statist. Prob. 1961 1 547 561
15. Furuichi S. Information theoretical properties of Tsallis entropies J. Math. Phys. 2006 47 023302 10.1063/1.2165744
16. Manije S. Gholamreza M. Mohammad A. Conditional Tsallis Entropy Cyb. Inf. Technol. 2013 13 37 42 10.2478/cait-2013-0012
17. Heinrich F. Ramzan F. Rajavel F.A. Schmitt A.O. Gültas M. MIDESP: Mutual Information-Based Detection of Epistatic SNP Pairs for Qualitative and Quantitative Phenotypes Biology 2021 10 921 10.3390/biology10090921 34571798
18. Oggier F. Datta A.A. Renyi entropy driven hierarchical graph clustering PeerJ Comput. Sci. 2021 7 e366 10.7717/peerj-cs.366
19. Tao M. Wang S. Chen H. Wang X. Information space of multi-sensor networks Inf. Sci. 2021 565 128 245 10.1016/j.ins.2021.02.059
20. Jozsa R. Schlienz J. Distinguishability of states and von Neumann entropy Phys. Rev. A 2000 62 012301 10.1103/PhysRevA.62.012301
21. Hassani H. Unger S. Entezarian M. Information content measurement of esg factors via entropy and its impact on society and security Information 2021 12 391 10.3390/info12100391
22. Shannon C.E. Communication theory of secrecy systems Bell Syst. Tech. J. 1949 28 656 715 10.1002/j.1538-7305.1949.tb00928.x
23. Bhotto M.Z.A. Antoniou A. A new normalized minimum-error entropy algorithm with reduced computational complexity Proceedings of the 2009 IEEE International Symposium on Circuits and Systems Taipei, Taiwan 24–27 May 2009 2561 2564 10.1109/ISCAS.2009.5118324
24. Teixeira A. Matos A. Souto A. Antunes L. Entropy measures vs. Kolmogorov complexity Entropy 2011 13 595 611 10.3390/e13030595
25. Teixeira A. Souto A. Matos A. Antunes L. Entropy measures vs. algorithmic information Proceedings of the 2010 IEEE International Symposium on Information Theory Austin, TX, USA 13–18 June 2010 1413 1417 10.1109/ISIT.2010.5513643
26. Edgar T. Manz D. Chapter 2-Science and Cyber Security Research Methods for Cyber Security Syngress Amsterdam, The Netherlands 2017 33 62
27. Huang L. Shen Y. Zhang G. Luo H. Information system security risk assessment based on multidimensional cloud model and the entropy theory Proceedings of the 2015 IEEE 5th International Conference on Electronics Information and Emergency Communication Beijing, China 14–16 May 2015 11 15
28. Lu R. Shen H. Feng Z. Li H. Zhao W. Li X. HTDet: A clustering method using information entropy for hardware Trojan detection Tsinghua Sci. Technol. 2021 26 48 61 10.26599/TST.2019.9010047
29. Firman T. Balázsi G. Ghosh K. Building Predictive Models of Genetic Circuits Using the Principle of Maximum Caliber Biophys J. 2017 113 2121 2130 10.1016/j.bpj.2017.08.057 29117534
30. Jost L. Entropy and diversity Oikos 2006 113 363 375 10.1111/j.2006.0030-1299.14714.x
31. Roach TNF Use and Abuse of Entropy in Biology: A Case for Caliber Entropy 2020 22 1335 10.3390/e22121335 33266519
32. Simpson E. Measurement of diversity Nature 1949 163 688 10.1038/163688a0
33. Yin Y. Shang P. Weighted permutation entropy based on different symbolic approaches for financial time series Phys. A Stat. Mech. Its Appl. 2016 443 137 148 10.1016/j.physa.2015.09.067
34. Castiglioni P. Parati G. Faini A. Information-Domain Analysis of Cardiovascular Complexity: Night and Day Modulations of Entropy and the Effects of Hypertension Entropy 2019 21 550 10.3390/e21060550 33267264
35. Polizzotto N.R. Takahashi T. Walker C.P. Cho R.Y. Wide Range Multiscale Entropy Changes through Development Entropy 2016 18 12 10.3390/e18010012
36. Prabhu K.P. Martis R.J. Diagnosis of Schizophrenia using Kolmogorov Complexity and Sample Entropy Proceedings of the 2020 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT) Bangalore, India 2–4 July 2020 1 4 10.1109/CONECCT50063.2020.9198472
37. Fehr S. Berens S. On the Conditional Rényi Entropy IEEE Trans. Inf. Theory 2014 60 6801 6810 10.1109/TIT.2014.2357799
38. Teixeira A. Matos A. Antunes L. Conditional Rényi Entropies IEEE Trans. Inf. Theory 2012 58 4273 4277 10.1109/TIT.2012.2192713

