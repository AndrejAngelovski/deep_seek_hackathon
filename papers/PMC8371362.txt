
==== Front
R Soc Open Sci
RSOS
royopensci
Royal Society Open Science
2054-5703
The Royal Society

10.1098/rsos.201976
rsos201976
10037Computer Science and Artificial Intelligence
Research Articles
Personality first in emotion: a deep neural network based on electroencephalogram channel attention for cross-subject emotion recognition
Personality first in emotion: a deep neural network based on electroencephalogram channel attention for cross-subject emotion recognition
Tian Zhihang 1 2 †
Huang Dongmin 1 2
Zhou Sijin 1 2
http://orcid.org/0000-0001-9459-7984
Zhao Zhidan zzhidanzhao@gmail.com
1 2
Jiang Dazhi dzjiang@stu.edu.cn
1 2 †
1 Department of Computer Science, School of Engineering, Shantou University, Shantou 515063, People’s Republic of China
2 Key Laboratory of Intelligent Manufacturing Technology (Ministry of Education), Shantou University, Shantou 515063, People’s Republic of China
† Contributions: Zhihang Tian and Dazhi Jiang contributed equally to this work.

18 8 2021 Auguest 18, 2021
8 2021
8 8 20197615 11 2020 November 15, 2020
12 7 2021 July 12, 2021
© 2021 The Authors.
2021
https://creativecommons.org/licenses/by/4.0/ Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.

In recent years, more and more researchers have focused on emotion recognition methods based on electroencephalogram (EEG) signals. However, most studies only consider the spatio-temporal characteristics of EEG and the modelling based on this feature, without considering personality factors, let alone studying the potential correlation between different subjects. Considering the particularity of emotions, different individuals may have different subjective responses to the same physical stimulus. Therefore, emotion recognition methods based on EEG signals should tend to be personalized. This paper models the personalized EEG emotion recognition from the macro and micro levels. At the macro level, we use personality characteristics to classify the individuals’ personalities from the perspective of ‘birds of a feather flock together’. At the micro level, we employ deep learning models to extract the spatio-temporal feature information of EEG. To evaluate the effectiveness of our method, we conduct an EEG emotion recognition experiment on the ASCERTAIN dataset. Our experimental results demonstrate that the recognition accuracy of our proposed method is 72.4% and 75.9% on valence and arousal, respectively, which is 10.2% and 9.1% higher than that of no consideration of personalization.

cross subject
, electroencephalogram emotion recognition
, personality first
, deep neural network
National Natural Science Foundation of China http://dx.doi.org/10.13039/501100001809 61902231 61902232 Natural Science Foundation 2019A1515010943 Scientific Research Foundation of Shantou University NTF19015 Li Ka Shing Foundation http://dx.doi.org/10.13039/100007421 2020LKSFG04D 2020LKSFG09D Key Project of Basic and Applied Basic Research 2018KZDXM035 Special Projects in Artificial Intelligence 2019KZDZX1030
==== Body
1. Introduction

Emotion recognition plays an important role in interpersonal communication and human–computer interaction, and the research of emotion recognition has been developed for decades. Generally, human emotions can be predicted by three methods: non-verbal behaviour methods (such as facial expression recognition, action recognition, etc.) [1], speech behaviour methods (such as text emotion recognition, conversation emotion analysis, etc.) [2], and methods based on physiological signals (such as electroencephalogram (EEG)-based emotion recognition, electrocardiogram-based emotion recognition, etc.) [3]. Because of the complexity of human emotion expression, many emotion recognition methods use physiological signals such as EEG, electrooculogram (EOG), electromyogram (EMG), Galvanic skin response (GSR), respiration and blood pressure etc. Koelstra et al. [4] analysed the mapping relationship between blood volume pressure, respiratory rate, skin temperature, EOG and emotion caused by 40 music videos. Subramanian et al. [5] studied the binary emotion recognition based on the ASCERTAIN dataset, which include physiological characteristics such as GSR, EEG, electrocardiogram (ECG) and facial landmark trajectory (EMO) etc. EEG stands out from many signals because of its high time resolution, large real-time difference and close connection with different emotional states of the human brain. It has been proved that EEG signals can effectively identify different emotions [6–10]. For computational model problems, researchers have proposed many methods and models to recognize emotion through EEG signals [11–15]. Among the numerous methods of EEG emotion recognition, it is worth noting that in recent years, the method based on deep learning has achieved a dominant position in improving the performance of EEG emotion recognition. For example, Zheng & Lu [11] introduced deep belief networks (DBNs) to construct an emotion recognition model based on EEG. Pandey & Seeja [16] proposed a multilayer perceptron neural network for independent emotion recognition. Song et al. [17] constructed graph relationships based on multi-channel EEG data and convoluted the graph to extract features for classification. Li et al. [18] established a new hierarchical spatio-temporal neural network based on brain regions for EEG emotion recognition. In addition, more and more scholars pay attention to the research of EEG channel distribution and channel selection algorithm which are closely related to emotion. Ansari-Asl et al. [19] proposed a channel selection algorithm based on the synchronization likelihood method. Five channels were selected from 64 emotional EEG channels, and the classification effect was not significantly reduced when identifying positive, medium and negative emotional states. Zhang et al. [20] report a channel selection algorithm based on ReliefF and applied it to the classification of four emotional states: pleasure, fear, sadness and ease. To consider the influence of personality on emotion, the personality theory widely accepted by scholars is the Big Five personality model proposed by Lew Goldberg in 1990 [21]. They believe that human personality can be described in five dimensions—openness, conscientiousness, extraversion, agreeableness and neuroticism (OCEAN). Vinciarelli & Mohammadi [22,23] have twice carried on the comprehensive elaboration to the personality computation. Stemmler & Wacker [24] focused on personality, emotion and individual differences in physiological responses. Winter & Kuiper [25] conducted extensive research on the relationship between personality and emotion in social psychology.

Although some progress has been made, previous works have only considered personality as an undifferentiated, insignificant and independent model feature and do not consider the internal connection between personality and physiological signals. In social psychology, the relationship between personality and emotion has been widely studied [26–28]. In social groups, different people have different cognition and personality, and they may have different reactions under the same emotional stimulus [29]. More importantly, the existing research does not further explore the influence of personality on emotional response. As mentioned above, we have reason to believe that personality is a good indicator to measure the difference of human emotional response under the same emotional stimulus [30]. Hence, we present a deep neural network based on EEG channel attention (DNNECA) model, which attaches great importance to the role of personality in emotion analysis from both the macro and micro points of view comprehensively. Basically, from a macro point of view, personality is taken as the corresponding emotional characteristic of each subject under the same emotional stimulus, and the participants are divided into different groups by calculating the differences between personality. This division process is intuitive and adaptable. Therefore, according to this idea, we apply clustering algorithm to classify the subjects according to their personality. Then explore the influence of personalization on emotion recognition. Specifically, we employ a deep learning model to extract the temporal and spatial feature information of EEG and reveal that the contribution of different EEG channels to emotion recognition is different from the micro perspective. Finally, in order to better analyse personalized emotions, we introduce a channel weight layer, which can establish the internal connection mapping between EEG channels and EEG emotions to highlight the role of EEG channels in EEG emotion recognition.

The remainder of this paper is organized as follows: In §2, we specify the data preprocessing, data structure construction and the deep learning model based on EEG channel attention combined with personality first (PF). In §3, we conduct extensive experiments to evaluate the proposed method for EEG emotion recognition. Finally, in §4, we discuss and conclude the paper.

2. Material and methods

2.1. Dataset

To the best of our knowledge, ASCERTAIN [5] is the only released and published dataset that links personality and emotional state through physiological reactions, as shown in table 1. Fifty-eight college students (21 female, average age = 30) who are fluent in English and frequent visitors to Hollywood movies were invited to watch 36 movie clips of 51–127 s from [31]. The movie clips are shown to be uniformly distributed (nine clips per quadrant) over the arousal-valence (AV) plane. During video viewing, several commercial sensors are used to record physiological signals. After watching each segment, participants were asked to label the AV ratings reflecting their affective impression with a seven-point scale, i.e. −3 (very negative) to 3 (very positive) scale for V, and 0 (very boring) to 6 (very exciting) scale for A. Personality measures for big-five dimensions were also compiled using a big-five marker scale questionnaire [32]. Table 1. The details of the ASCERTAIN dataset.

attribute	description	
subject	58 college students (21 female, average age = 30)	
stimulant	36 movie clips	
number of trials	36	
length of each trial	51–127 s	
recorded EEG signals	8 electrode and 32Hz sampling rate	
rating scales	a seven-point scale was used with a −3 to 3 scale for valence, a 0 to 6 scale for arousal, engagement (did not pay attention−totally attentive), liking (I hated it–I loved it) and familiarity (never seen it before–remember it very well)	
EEG data	trials*channels*data	
label	trials*labels (arousal, valence, liking, engagement and familiarity)	

2.2. Data preprocessing

In order to understand the impurities and noise in the original EEG data in the ASCERTAIN dataset, the dataset needs to be preprocessed. First, for the EEG record segments with data loss and record failure, we remove them directly. Then, the common average reference is applied to the original EEG signals. Next, each EEG electrode signal is filtered with 1 Hz high-pass filter and 16 Hz low-pass filter. Finally, the EOG and EMG artefacts are removed from the EEG data by independent component analysis (ICA).

2.3. Data structure construction

Adapt to the proposed EEG emotion recognition model properly, this section reconstructs the representation structure of the EEG signal based on the previous section of EEG signal preprocessing. Generally, for a group of experimental EEG signals, the data are recorded from different EEG channels, which is the data matrix of M × L, where M is the number of EEG channels and L is the length of EEG record. Therefore, a set of two-dimensional step windows with width of ω, height of M′ and step length of s is used to cut the data matrix to obtain U group data units. Figure 1 presents the structure after data cutting, where U can be calculated as follows:2.1 { U=(L−ω)/s+1 M′=M ω≤L.

Figure 1. Representation structure of EEG data reconstructed into data unit.

Given an EEG signal trial completed by the above processing, the adjacent N = 9 segments were regarded as one experimental sample. For each EEG segment, we extract the differential entropy (DE) features [33] of five EEG bands (δ band (1–4 Hz), θ band (4–8 Hz), low α band (8–11 Hz), high α band (11–14 Hz), β band (14–16 Hz)) for each EEG electrode signal.

2.4. Personality-based subjects clustering in the macro level

Our goal is to realize cross-subject emotion recognition from EEG signals considering personality. We use k-means clustering algorithm to construct the relationship between subjects according to the following rules: 2.2 c(i)=argmin j  ∥p(i)−μj∥2,

2.3 μj=∑i=1Pδ(c(i),j)p(i)∑i=1Pδ(c(i),j)

2.4 andδ(c(i),j)={1if c(i)=j0otherwise,

where p(i) is the personality of the i-th subject, μj (j=1,2, ⋯,P) is the randomly selected P cluster centroids, and c(i) is the category of the i-th subject. Here, μj is updated by equation (2.4).

2.5. DNNECA with personality first in the micro level

In this section, we report in detail the EEG channel attention model and the method applied to the cross-subject EEG emotion recognition. Figure 2 illustrates the framework of EEG channel attention model. The raw EEG is preprocessed to extract the DE feature of frequency band. Attention mechanism is aimed at different channels, which have different effects on emotion. Finally, the extracted spatio-temporal information is used as classification feature. Figure 2. Frame diagram based on EEG channel attention model. First, perform corresponding preprocessing on the original data, including clustering operations. Then, EEG spatial information is explored by constructing the relationship between channels, exploring the effects of channels themselves on emotions, and capturing the dynamic information of EEG signals to obtain EEG time information.

Here, we provide the implementation of the model in detail. As EEG signal is preprocessed, then the data structure is reconstructed, and the frequency band feature is extracted. Let X=[x1,x2,…,xN]∈RF×M×N be an experimental sample, where xi is the band feature extracted from the i-th segment of the experimental sample, F, M, N are the number of extracted band features, the number of EEG channels, and the number of EEG segments in an experimental sample, respectively. Initially, we model a neural network layer for each channel to obtain the channel deep information. After learning the channel deep information, the channel-attention layer is used to learn the weight information of channel importance, and then the learned weighted channel information is input into a bidirectional long short-term memory (BiLSTM) network to get the spatial information [18,34]. Finally, the spatial information obtained from each segment is used as input to learn temporal information in BiLSTM, and the final emotion recognition is completed by the classifier. The specific process of these studies is described as follows:

(1) Channel deep information learning: For each EEG segment, let xi = [m i1, mi2, …, miM] and mij denote the frequency band features of F dimension extracted from j-th channel, and for segment xi, each column corresponds to an EEG channel. We model a neural network layer for each channel to obtain the channel deep information. The channel deep information learning can be expressed as2.5 aij=Mjmij+bj,j=1,2,…,M

and2.6 Ai=[ai1,ai2,…,aiM],

where Mj indicates learnable transformation matrices and bj is the bias.

(2) Channel weight learning: Studies have shown that different emotions are closely related to the distribution and selection of EEG channels [19,20]. As the contribution of different EEG channels to emotion recognition is different, we introduce a channel weighting layer to highlight the role of EEG channels in EEG emotion recognition. A channel weight matrix W = {wjk} is chosen to weight each EEG channel, i.e.2.7 W=(Q tanh(HAi+B))T,

2.8 w jk=exp⁡(w jk)∑i=1Mexp⁡(wik)

2.9 andA^i=AiW,

where Q and H are learnable transformation matrices and B is the bias matrix. The column of W is normalized to the weight value by equation (2.8). Obviously, the larger the wjk, the more important the k-th EEG channel.

(3) Spatial information learning: The weighted channel information obtained above is used to capture potential structural information. Here, we use BiLSTM network to capture the spatial information of EEG, which can be formulated as follows:2.10 Ci=BL(A^i)=[ci1,ci2, …,ciM],

where BL is the BiLSTM operation. For the spatial information sequence of EEG, we compress it into a sequence of length K according to the following rules:2.11 c^ik=ReLU(∑ j=1Mg jkcij+bc)

and2.12 C^i=[c^i1,c^i2, …,c^iK]

where Gc = [gjk] is the parameter matrix, bc is the bias and ReLU is the activation function.

(4) Temporal information learning: We connect the column vectors of C^i into a vector, represented by di. The final temporal information matrix E is calculated as follows:2.13 di=[(c^i1)T,(c^i2)T, …,(c^iK)T]T,

2.14 D=[d1,d2, …,dN]

2.15 andE=BL(D)=[e1,e2, …,eN].

(5) Classifier: Based on the final temporal information, we use a simple linear transformation to predict the class label of the input EEG experimental sample X, which can be expressed as2.16 y=GyeN+by=[y1,y2, …,yT],

where Gy and by , respectively, represent the transformation matrix and bias, and T is the number of classes. The elements in output y are sent into softmax function for emotional recognition, i.e.2.17 P(t∣X)=exp⁡(yt)∑i=1Texp⁡(yi),t=1, …,T,

where P(t|X) is the probability that input X is predicted as class t.

Suppose that there are training samples, represented by m matrices Xi(i = 1, …, m). The loss function of the classifier can be expressed as2.18 L(Xi)=−∑t=1mτ(li,t)×log⁡P(t∣Xi)

and2.19 J(X1,…,Xm)=1m∑i=1mL(Xi),

where li represents the ground truth label of Xi, which τ(li, t) can be expressed as2.20 τ(li,t)={1if li=t0otherwise.

By minimizing the loss function equation (2.19), we can maximize the probability of correctly predicting the emotion class of each training sample. For the test set, we take the category of the maximum probability value as the prediction category of the sample. It is worth noting that the EEG experimental samples trained and tested may come from different subjects when dealing with EEG emotion recognition. In this condition, the emotion recognition model based on training data may not be suitable for test data. In order to realize the cross-subject emotion recognition, we introduce the personality first to divide the subjects into P categories. Moreover, to reduce the individual differences in emotion recognition, an emotion recognition model is trained for each group to make predictions.

3. Results and analysis

In order to study the clustering effect of personality, we studied the clustering effect of k-means under different clustering numbers. Figure 3 illustrates an anti-correlation relationship between the number of selected clusters and the sum of squares within the cluster, that is, as the number of cluster increases, distortion decreases. As can be seen, when the number of selected clusters is less than five, the curve descending speed is very fast, but when the number of selected clusters is greater than five, the curve descending speed becomes slower. This indicates that five is the most appropriate cluster number. Figure 3. Clustering effect of different cluster number. The horizontal axis represents the number of selected clusters, and the vertical axis represents the sum of squares within the cluster.

Figure 4 presents the relationship between subject number and class when the number of clusters is five. It can be observed from figure 4 that there is basically no clear correlation between subject number and class. Within the cluster are subjects with similar personality, so they may have similar emotions under the same emotional stimulation. Figure 4. Clustering result of the subjects’ personality. The horizontal axis represents the number of subject, and the vertical axis represents the category of subjects.

To highlight that the contribution of different EEG channels to emotion recognition is different, we introduce the channel attention layer into the model. The attention matrix of the five groups is shown in figure 5. In addition, the colour of attention matrix represents the contribution of channel. It can be observed that there are differences in the effect of EEG channel on emotion recognition for different personalized population. Similar to [5], we divide it into two parts based on the median value of arousal and valence, and finally use the recognition accuracy as the evaluation standard. We carry out experiments using the channel attention cross-subjects emotion recognition method based on personality first proposed in this paper. Figure 5. Attention matrix of the five groups. Each column corresponds to a channel, and each row corresponds to a time step in BiLSTM. The depth of colour represents the weight of attention. The larger the number of the EEG channel, the greater the contribution rate of the channel to emotion recognition.

In order to facilitate comparison, we also conduct experiments on emotion recognition without personality first. At the same time, support vector machine (SVM) and XGBoost [35] are used to do the same experiment as our method. During the experiment, we divide the population into different groups, then train and predict each group individually, and finally find the average of the experimental results of all groups. Tenfold cross validation is used for emotion recognition of non-cross subject. For cross-subject emotion recognition, leave-one-subject-out verification strategy is used to evaluate the recognition performance. The test results of non-cross-subject recognition accuracy are shown in table 2. The test results of cross-subject recognition accuracy are shown in table 3. Table 2. Non-cross-subject emotion recognition results with and without personality first in terms of recognition accuracy (%), where No PF indicates without personality first, where -C indicates without channel weighting layer.

	no PF	PF	
	SVM	XGB	Our-C	Our	SVM	XGB	Our-C	Our	
valence	57.3	60.5	64.3	68.6	60.1	63.4	68.4	73.8	
arousal	63.6	65.9	68.7	73.3	65.7	68.5	72.3	77.5	

Table 3. Cross-subject emotion recognition results with and without personality first in terms of recognition accuracy (%), where No PF indicates without personality first, where -C indicates without channel weighting layer.

	no PF	PF	
	SVM	XGB	Our-C	Our	SVM	XGB	Our-C	Our	
valence	53.8	55.2	58.9	62.2	57.6	59.9	66.2	72.4	
arousal	56.2	58.8	62.7	66.8	61.2	64.7	70.2	75.9	

In general, the results of our proposed method are better than traditional SVM and XGBoost classifiers with or without personality. Meanwhile, in terms of valence and arousal, the accuracy of the three methods based on personality first is higher than emotion recognition without it. Table 2 demonstrates the experimental results in non-cross subjects. As can be seen, without personality first, our proposed method improves performance of SVM and XGBoost by 11.3%, 8.1% on valence, by 9.7%, 7.4% on arousal, and the performance gain of our method (Our) over the method without channel weighting layer (Our-C) is 4.3% on valence, and 4.6% on arousal, respectively. In addition, with personality first, our proposed method improves performance of SVM and XGBoost by 13.7%, 10.4% on valence, by 11.8%, 9% on arousal, and the performance gain of our method (Our) over the method without channel weighting layer (Our-C) is 5.4% on valence, and 5.2% on arousal, respectively.

Table 3 summarizes the experimental results considering cross subjects. As can be seen, without personality first, our proposed method improves performance of SVM and XGBoost by 8.4%, 7% on valence, by 10.6%, 8% on arousal, and the performance gain of our method (Our) over the method without channel weighting layer (Our-C) is 3.3% on valence, and 4.1% on arousal, respectively. Additionally, with personality first, our proposed method improves performance of SVM and XGBoost by 14.8%, 12.5% on valence, by 14.7%, 11.2% on arousal, and the performance gain of our method (Our) over the method without channel weighting layer (Our-C) is 6.2% on valence, and 5.7% on arousal, respectively.

Comparing tables 2 and 3, it is obvious that a significant improvement was obtained in the majority of cases. It is evident that the cross subject emotion recognition based on personality first effectively improves the accuracy of the model, which further proves the effectiveness of the personality first method. We consider personality factors which associate different subjects with similar personality traits. It makes the potential correlation among different subjects effectively mined.

4. Discussion and conclusion

With the rise of emotion recognition research, more and more scholars pay attention to the research of EEG channel distribution and channel selection algorithm which are closely related to emotion. Much research has focused on the relationship between personality and emotion [26–28] and studied the reactions to the same emotional stimulus [29]. In this paper, we have considered the personality as a indicator to measure the difference of human emotional response under the same emotional stimulus. Hence, we propose to realize the EEG cross-subject emotion recognition through personality first, and provide a channel attention cross-subject emotion recognition model DNNECA with personality first mechanism. The spatial-temporal EEG information is extracted by considering the weight of channel combined with personality first to complete the cross-subject emotion recognition. Due to the differences between individuals, to effectively carry out the cross-subject emotion recognition, we select individuals with similar personality through personality first, and learn the channel deep information, channel weight information, spatial information and temporal information of them. Additionally, extensive experiments provide compelling evidence that our method is significantly better than that without personality first and can be extended to new subjects with known personality and EEG signals. An important question for future studies is to add some other physiological signals that can evoke emotion, combined with video stimulation content to do personalized emotion recognition. Our methods can be applied to a wide range of areas from emotions, human behaviours to multiplex networks analysis, etc. [10].

Supplementary Material

Click here for additional data file.

Acknowledgements

The authors thank anonymous reviewers for their very detailed and helpful review and thanks very much for using ASCERTAIN Dataset.

Data accessibility

Our article is based on the ASCERTAIN Dataset. If you want to use this data, please refer to the official instructions: http://mhug.disi.unitn.it/wp-content/ASCERTAIN/ascertain.html.

Authors' contributions

Z.T., D.H. and S.Z. did the analytical and numerical calculations. Z.T., D.H. and S.Z. analysed the empirical data. Z.T., Z.Z. and D.J. wrote the manuscript.

Competing interests

The authors declare no conflict of interest. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.

Funding

This work was supported by National Natural Science Foundation of China (61902232, 61902231), Natural Science Foundation of Guangdong Province (2019A1515010943), Key Project of Basic and Applied Basic Research of Colleges and Universities in Guangdong Province (Natural Science) (2018KZDXM035), The Basic and Applied Basic Research of Colleges and Universities in Guangdong Province (Special Projects in Artificial Intelligence: 2019KZDZX1030 and 2021A1515012294), the Scientific Research Foundation of Shantou University (grant no. NTF19015) and 2020 Li Ka Shing Foundation Cross-Disciplinary Research (grant nos. 2020LKSFG04D, 2020LKSFG09D).
==== Refs
References

1. EdwardsJ, JacksonHJ, PattisonPE. 2002 Emotion recognition via facial expression and affective prosody in schizophrenia: a methodological review. Clin. Psychol. Rev. 22 , 789-832. (10.1016/S0272-7358(02)00130-7)12214327
2. ChuangZJ, WuCH. 2004 Multi-modal emotion recognition from speech and text. Int. J. of Computational Linguistics & Chinese Language Processing: Special Issue on New Trends of Speech and Language Processing9, 45–62.
3. JerrittaS, MurugappanM, NagarajanR, WanK. 2011 Physiological signals based human emotion recognition: a review. In 2011 IEEE 7th Int. Colloquium on Signal Processing and its Applications, pp. 410–415. IEEE.
4. KoelstraS, MuhlC, SoleymaniM, LeeJS, YazdaniA, EbrahimiT, PunT, NijholtA, PatrasI. 2011 Deap: a database for emotion analysis; using physiological signals. IEEE Trans. Affect. Comput. 3 , 18-31. (10.1109/T-AFFC.2011.15)
5. SubramanianR, WacheJ, AbadiMK, VieriuRL, WinklerS, SebeN. 2016 ASCERTAIN: emotion and personality recognition using commercial sensors. IEEE Trans. Affect. Comput. 9 , 147-160. (10.1109/TAFFC.2016.2625250)
6. SammlerD, GrigutschM, FritzT, KoelschS. 2007 Music and emotion: electrophysiological correlates of the processing of pleasant and unpleasant music. Psychophysics 44 , 293-304. (10.1111/j.1469-8986.2007.00497.x)
7. KnyazevGG, Slobodskoj-PlusninJY, BocharovAV. 2010 Gender differences in implicit and explicit processing of emotional facial expressions as revealed by event-related theta synchronization. Emotion 10 , 678. (10.1037/a0019175)21038950
8. MathersulD, WilliamsLM, HopkinsonPJ, KempAH. 2008 Investigating models of affect: relationships among EEG alpha asymmetry, depression, and anxiety. Emotion 8 , 560. (10.1037/a0012811)18729586
9. BajajV, PachoriRB. 2014 Human emotion classification from EEG signals using multiwavelet transform. In 2014 Int. Conf. on Medical Biometrics, pp. 125–130. IEEE.
10. JonauskaiteD, WickerJ, MohrC, DaelN, HavelkaJ, Papadatou-PastouM, ZhangM, OberfeldD. 2019 A machine learning approach to quantify the specificity of colour-emotion associations and their cultural differences. R. Soc. Open Sci. 6 , 190741. (10.1098/rsos.190741)31598303
11. ZhengWL, LuBL. 2015 Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks. IEEE Trans. Auton. Ment. Dev. 7 , 162-175. (10.1109/TAMD.2015.2431497)
12. ZhengW. 2016 Multichannel EEG-based emotion recognition via group sparse canonical correlation analysis. IEEE Trans. Cogn. Dev. Syst. 9 , 281-290. (10.1109/TCDS.2016.2587290)
13. LiY, ZhengW, CuiZ, ZongY, GeS. 2019 EEG emotion recognition based on graph regularized sparse linear regression. Neural. Process. Lett. 49 , 555-571. (10.1007/s11063-018-9829-1)
14. LiX, SongD, ZhangP, ZhangY, HouY, HuB. 2018 Exploring EEG features in cross-subject emotion recognition. Front. Neurosci. 12 , 162. (10.3389/fnins.2018.00162)29615853
15. CaiH, ZhangX, ZhangY, WangZ, HuB. 2018 A case-based reasoning model for depression based on three-electrode EEG data. IEEE Trans. Affect. Comput. 11 , 383-392. (10.1109/TAFFC.2018.2801289)
16. PandeyP, SeejaK. 2019 Emotional state recognition with EEG signals using subject independent approach. In Data Science and Big Data Analytics, pp. 117–124. New York, NY: Springer.
17. SongT, ZhengW, SongP, CuiZ. 2018 EEG emotion recognition using dynamical graph convolutional neural networks. IEEE Trans. Affect. Comput. 11 , 532-541. (10.1109/TAFFC.2018.2817622)
18. LiY, ZhengW, WangL, ZongY, CuiZ. 2019 From regional to global brain: a novel hierarchical spatial-temporal neural network model for EEG emotion recognition. IEEE Trans. Affect. Comput. (10.1109/TAFFC.2019.2922912)
19. Ansari-AslK, ChanelG, PunT. 2007 A channel selection method for EEG classification in emotion assessment based on synchronization likelihood. In 2007 15th European Signal Processing Conf., pp. 1241–1245. IEEE.
20. ZhangJ, ChenM, ZhaoS, HuS, ShiZ, CaoY. 2016 ReliefF-based EEG sensor selection methods for emotion recognition. Sensors 16 , 1558. (10.3390/s16101558)
21. GoldbergLR. 1990 An alternative ‘description of personality’: the big-five factor structure. J. Pers. Soc. Psychol. 59 , 1216. (10.1037/0022-3514.59.6.1216)2283588
22. VinciarelliA, MohammadiG. 2014 A survey of personality computing. IEEE Trans. Affect. Comput. 5 , 273-291. (10.1109/TAFFC.2014.2330816)
23. VinciarelliA, MohammadiG. 2014 More personality in personality computing. IEEE Trans. Affect. Comput. 5 , 297-300. (10.1109/TAFFC.2014.2341252)
24. StemmlerG, WackerJ. 2010 Personality, emotion, and individual differences in physiological responses. Biol. Psychol. 84 , 541-551. (10.1016/j.biopsycho.2009.09.012)19800934
25. WinterKA, KuiperNA. 1997 Individual differences in the experience of emotions. Clin. Psychol. Rev. 17 , 791-821. (10.1016/S0272-7358(97)00057-3)9397338
26. CarverCS, SuttonSK, ScheierMF. 2000 Action, emotion, and personality: emerging conceptual integration. Pers. Soc. Psychol. Bull. 26 , 741-751. (10.1177/0146167200268008)
27. ShiotaMN, KeltnerD, JohnOP. 2006 Positive emotion dispositions differentially associated with Big Five personality and attachment style. J. Posit. Psychol. 1 , 61-71. (10.1080/17439760500510833)
28. JohnOP, GrossJJ. 2004 Healthy and unhealthy emotion regulation: personality processes, individual differences, and life span development. J. Pers. 72 , 1301-1334. (10.1111/j.1467-6494.2004.00298.x)15509284
29. AbadiMK, CorreaJAM, WacheJ, YangH, PatrasI, SebeN. 2015 Inference of personality traits and affect schedule by analysis of spontaneous reactions to affective videos. In 2015 11th IEEE Int. Conf. and Workshops on Automatic Face and Gesture Recognition (FG), vol. 1, pp. 1–8. IEEE.
30. Al OsmanH, FalkTH. 2017 Multimodal affect recognition: current approaches and challenges. In Emotion and attention recognition based on biological signals and images (ed. SAHosseini), pp. 59-86. Rijeka, Croatia: InTech.
31. AbadiMK, SubramanianR, KiaSM, AvesaniP, PatrasI, SebeN. 2015 DECAF: MEG-based multimodal database for decoding affective physiological responses. IEEE Trans. Affect. Comput. 6 , 209-222. (10.1109/TAFFC.2015.2392932)
32. PeruginiM, Di BlasL. 2002 Analyzing personality related adjectives from an eticemic perspective: the big five marker scales (BFMS) and the Italian AB5C taxonomy. Big Five Assessment. 281-304.
33. ShiLC, JiaoYY, LuBL. 2013 Differential entropy feature for EEG-based vigilance estimation. In 2013 35th Annual Int. Conf. of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 6627–6630. IEEE.
34. YuZ, RamanarayananV, Suendermann-OeftD, WangX, ZechnerK, ChenL, TaoJ, IvanouA, QianY. 2015 Using bidirectional LSTM recurrent neural networks to learn high-level abstractions of sequential features for automated scoring of non-native spontaneous speech. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pp. 338–345. IEEE.
35. ChenT, GuestrinC. 2016 XGBoost: a scalable tree boosting system. In Proc. of the 22nd ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining KDD ’16, pp. 785–794. Association for Computing Machinery.

